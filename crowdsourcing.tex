    Many machine learning algorithms rely on having accurate ground truth for relatively complicated data sets. These ``ground truth" labels are usually delegated to humans on some sort of system like Amazon's Mechanical Turk. Unfortunately due to the circular nature of the problem there is no way to verify the accuracy of these labels. Much of the previous work in this field revolved around the David-Skene Estimator \cite{dawid1979maximum}. The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. Moreover the iterative nature of EM gives no bound on the speed of convergence.  Recent work by Zhang et al. \cite{zhang2014spectral} presents a new spectral method for ground truth label generation by using spectral methods to estimate the confusion matrix for the labelers, and provides a good theoretical bound on the rate of convergence. In fact in the papers results section, it claims it reaches very close to a global optimum in one iterate of EM. For the sake of simplicity we will consider only the two class classification case, the generalization to k classes is quite natural.

\subsection{Problem Statement}
Consider an experiment where $M$ labelers assign labels to $N$ samples from the set: $\left\{[0,1],[1,0]\right\}$ (negative,positive). I will refer to these vectors as $N$ and $P$ respectively. Note though our labels represent scalars we represent them by the standard basis vectors of $\mathcal{R}^{2}$. This will simplify our later analysis. We will denote labeler $m$'s label of sample $n$ as $z_{mn}$. 


\subsubsection{Assumptions}
\begin{enumerate}
\item
The true label $y_{i}$ of item
$i \in \left\{0,1....N\right\} $ is assumed to be sampled from a probability
distribution $Pr(y_{i} = 0)$ = $w_{0}$ where $w_{0}$ is the prior on label 0.

We define $W$ to be a diagonal $2 \times 2$ matrix:
\begin{align}
\begin{bmatrix}
w_{0} & 0 \\
0     & w_{1}
\end{bmatrix}
\end{align}

Naturally $w_{0} + w_{1} = 1$. \\

\item
Asssume that each of the labelers are conditionally independent given the true label. More formally:

$$Pr(z_{ij} = X | y_{j} = X)Pr(z_{hj} = X | y_{j} = X) = Pr(z_{ij}z_{hj} | y_{j} = X)$$

Where $i \neq h$.

Though this assumption may not hold for all instances of the labelling problem, it greatly simplifies our solution. We also assume that the vendors are more likely to get a sample correct than incorrect. We also assume every labeler labels every datapoint once.

\item
Finally we assume each worker is associated with a $2 \times 2$ confusion matrix. Where $(l,c)$-th entry represents $P(\text{Worker Label} = l| \text{True Label} = c)$. We will denote the confusion matrix for worker $j$ as  $C_{j}$.

\end{enumerate}

Now what we want is to estimate $y_{i}$ for all $i \in \left\{0,1....N\right\}$. And estimate $C_{j}$ for all $j \in \left\{0,1....M\right\}$. 
We would also like to estimate $W_{0}$ and $W_{1}$

\subsection{Dawid-Skene Estimator}
Let $\theta = (C_{0}...C_{M},w_{0},w_{1})$ be the parameters of our model.

We can define the likelihood of the data $P(z,y| \theta)$ as:

\begin{equation}  \label{eq:L}
\mathcal{L}(\theta) =
\displaystyle\prod\limits_{n=0}^{N} \displaystyle\prod\limits_{c=0}^{1}
\Big (w_{l} \displaystyle\prod\limits_{m=0}^{M} \displaystyle\prod\limits_{l=0}^{1} (C_{m}[l,c])^{z_{mn}[l]} \Big )^{y_{n}[c]}
\end{equation}

This can be easily verified using our above assumptions
We want: \\

\begin{equation} \label{eq:maxL}
\argmax_{\theta} \mathcal{L}(\theta)
\end{equation}

Unfortunately we don't know $y_{n}$ so we can't directly solve this optimization problem.
So instead we opt for an iterative algorithm with some nice theoretical guarentees. 


The intuition is quite simple. There two unknown quantities of interest: $y$ and $\theta$.
Given one the other is computable in some manner. So we will hold one of the two nknowns constant,
and compute the other. We will then hold the other unknown constant and repeat until convergence.

This algorithm is called the Expectation Maximization algorithm.




\subsection{Expectation Maximization Algorithm}

We sum out over $y_{n}$ in \label{eq:L} by taking a
conditional expectation.

\begin{equation} \label{eq:L'}
    \mathcal{L'}(\theta) = \E_{y | z, \theta^{(t)}}[L(\theta)]
\end{equation}

Where $\theta^{(t)}$ is the estimate of $\theta$ at timestep $t$. This step allows us to compute $y$ from a partiulcar 
$\theta^{(t)}$.

Next we solve

\begin{equation} \label{eq:maxL'}
    \theta^{(t+1)} = \argmax_{\theta} \mathcal{L'}(\theta)
\end{equation}


\begin{enumerate}
    \item Pick some initial estimates for the unknown $y$ call these $y^{(0)}$
    \item Solve \eqref{eq:maxL'} with the labels $y^{(0)}$ for an optimum $\theta^{(0)}$
    \item Calculate $L'(\theta^{(0)})$ Call these labels $y^{(1)}$
    \item Repeat steps 2 and 3 until convergence
\end{enumerate}


\subsection{Spectral Initialization}
